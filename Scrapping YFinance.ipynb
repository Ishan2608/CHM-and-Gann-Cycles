{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fde0bf-2285-41e0-981e-a64269e358ce",
   "metadata": {},
   "source": [
    "# Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549eb814-c72c-413d-b286-481bcfee50c0",
   "metadata": {},
   "source": [
    "1. Using a finance API, fetch the closing pricies of all stocks of the indian stock market for the past 10 years for each day the market was open.\n",
    "2. Organize data into a table. This is simple, first column will include the stock name/stock symbol. The rest of the columns are the dates, for example, the first column will be the last date of the last FY, then the second last, then third last, so on for last 10 years. The values of these columns are the closing prices for the day of the corresponsind stock.\n",
    "3. Then, perform mathematical formulation on the table, using Gann Cycles and CHM. The flow is simple. User inputs a start date and end date. The program will calculate the above methods for all stocks and fetch the list of top performing stocks in that time period/cycle.\n",
    "4. All of the above features will be implemented using various functions. All being called and used in a separate \"main\" function.\n",
    "5. Another separate function will fetch the list of poor performers in that time period.\n",
    "6. Future Optimizations: \n",
    "    - A function to update the data for the newer financial year will exist. This will simply check whether the data already exists or not. Of not, it will create the entire table including data till present date. if it exists, it simply fetches and updates the data from the last date in the data to current date. Normally, i would like to include data for complete financial years and not for financial years half way. \n",
    "    - A function will take a stock symbol, start date and ending date for a time period, calculate cycles and other relevant methods as mentioned above. Then it tell whether that stock will perform good in that time period or not by simply showing a set of numbers These number can be %change, and other  statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc0cce",
   "metadata": {},
   "source": [
    "# Imports and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b433595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install selenium pandas webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced1d1fe-f983-4f8c-8a96-f436775d3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af30bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "BSE_SUFFIX = \".BO\"\n",
    "NSE_SUFFIX = \".NS\"\n",
    "\n",
    "ALL_SYMBOLS = None\n",
    "symbols_defined = False\n",
    "\n",
    "all_data_exists = False\n",
    "combined_data = None\n",
    "\n",
    "bse_scrapped_data_exists = False\n",
    "bse_scrapped_data = None\n",
    "\n",
    "all_20_yrs_data = None\n",
    "all_20_yrs_data_exists = False\n",
    "\n",
    "gt_10_yrs_data = None\n",
    "gt_10_yrs_data_exists = False\n",
    "\n",
    "end_date = \"2024-03-31\"\n",
    "start_date = \"2003-04-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapped_data_folder = './scrapped_data'\n",
    "data_files_folder = './data_files'\n",
    "nse_list_path = f\"{data_files_folder}/NSE_STOCK_LIST.csv\"\n",
    "bse_list_path = f\"{data_files_folder}/BSE_STOCK_LIST.csv\"\n",
    "all_symbols_path = f\"{data_files_folder}/ALL_SYMBOLS.csv\"\n",
    "combined_stock_data_path = f\"{scrapped_data_folder}/combined_stock_data.csv\"\n",
    "bse_scrapped_data_path = f\"{scrapped_data_folder}/bse_scrapped_data.csv\"\n",
    "gt_10_yrs_data_path = f\"{scrapped_data_folder}/gt_10_yrs_data.csv\"\n",
    "all_20_yrs_data_path = f\"{scrapped_data_folder}/all_20_yrs_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f48edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    combined_data = pd.read_csv(combined_stock_data_path)\n",
    "    combined_data_exists = True\n",
    "    print(f\"Shape of DataFrame of 'combined_data' is {combined_data.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"'combined_stock_data.csv' File does not exist\")\n",
    "\n",
    "try:\n",
    "    bse_scrapped_data = pd.read_csv(bse_scrapped_data_path)\n",
    "    bse_scrapped_data_exists = True\n",
    "    print(f\"Shape of DataFrame of 'combined_data' is {bse_scrapped_data.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"'{bse_scrapped_data_path}' File does not exist\")\n",
    "\n",
    "try:\n",
    "    gt_10_yrs_data = pd.read_csv(gt_10_yrs_data_path)\n",
    "    gt_10_yrs_data_exists = True\n",
    "    print(f\"Shape of DataFrame of 'gt_10_yrs_data' is {gt_10_yrs_data.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"'gt_10_yrs_data.csv' File does not exist\")\n",
    "\n",
    "try:\n",
    "    all_20_yrs_data = pd.read_csv(all_20_yrs_data_path)\n",
    "    all_20_yrs_data_exists = True\n",
    "    print(f\"Shape of DataFrame of 'all_20_years_data' is {all_20_yrs_data.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"'all_20_years_data.csv' File does not exist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d4344-b0af-4999-9471-eca3e4dbc3b9",
   "metadata": {},
   "source": [
    "# Getting the List of Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd5c76-f338-4657-a29c-971f84ae95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSE_STOCKS = pd.read_csv(nse_list_path)\n",
    "print(f\"Column Headers of Table: {list(NSE_STOCKS.columns)}\")\n",
    "NSE_SYMBOLS = list(NSE_STOCKS['SYMBOL'])\n",
    "print(f\"Symbols: {NSE_SYMBOLS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960aa3f6-4020-467f-b060-daebb99d2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSE_STOCKS = pd.read_csv(bse_list_path)\n",
    "print(f\"Column Headers of Table: {list(BSE_STOCKS.columns)}\")\n",
    "BSE_SYMBOLS = list(BSE_STOCKS['Security Id'])\n",
    "print(f\"Symbols: {BSE_SYMBOLS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1575e35-970f-45e7-b1fd-e5e68e65b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the symbols into a set to remove duplicates\n",
    "try:\n",
    "    ALL_SYMBOLS = pd.read_csv(all_symbols_path)\n",
    "    symbols_defined = True\n",
    "except Exception as e:\n",
    "    print(f\"'ALL_SYMBOLS.csv' File does not exist\")\n",
    "\n",
    "if (not symbols_defined):\n",
    "    all_symbols_set = set(NSE_SYMBOLS + BSE_SYMBOLS)\n",
    "    all_symbols_set = set(NSE_SYMBOLS + BSE_SYMBOLS)\n",
    "    all_symbols_list = list(all_symbols_set)\n",
    "    all_symbols_list.sort()\n",
    "    # Convert the set back to a pandas Series\n",
    "    ALL_SYMBOLS = pd.DataFrame(list(all_symbols_set))\n",
    "    ALL_SYMBOLS.to_csv(all_symbols_path, index=False)\n",
    "\n",
    "\n",
    "print(ALL_SYMBOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99863df-642c-47e3-88ab-764299ea5219",
   "metadata": {},
   "source": [
    "# The Method that Scraps the Yahoo Finance Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71790c2a-a939-4e61-8792-0051e5eef4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_file = 'last_completed_stock.txt'  # File to store the last processed stock\n",
    "error_flag_file = 'error_flag.txt'          # File to store the error status\n",
    "output_file = bse_scrapped_data_path       # Final output CSV file\n",
    "\n",
    "# Initialize WebDriver\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run Chrome in headless mode\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Load the last completed stock index\n",
    "def load_last_completed_stock():\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as file:\n",
    "            return int(file.read())\n",
    "    return 0\n",
    "\n",
    "# Save the last completed stock index\n",
    "def save_last_completed_stock(stock_idx):\n",
    "    with open(progress_file, 'w') as file:\n",
    "        file.write(str(stock_idx))\n",
    "\n",
    "# Load the error flag status\n",
    "def load_error_flag():\n",
    "    if os.path.exists(error_flag_file):\n",
    "        with open(error_flag_file, 'r') as file:\n",
    "            return file.read() == 'True'\n",
    "    return False\n",
    "\n",
    "# Save the error flag status\n",
    "def save_error_flag(flag):\n",
    "    with open(error_flag_file, 'w') as file:\n",
    "        file.write(str(flag))\n",
    "\n",
    "# Remove the previous data for the stock (in case of error)\n",
    "def remove_previous_data(stock_symbol):\n",
    "    if os.path.exists(output_file):\n",
    "        df = pd.read_csv(output_file)\n",
    "        if stock_symbol in df.columns:\n",
    "            df = df.drop(columns=[stock_symbol])\n",
    "            df.to_csv(output_file, index=False)\n",
    "\n",
    "# Function to fetch stock data from Yahoo Finance\n",
    "def fetch_stock_data(driver, symbol, start_date, end_date):\n",
    "    stock_data = []\n",
    "    try:\n",
    "        url = f\"https://finance.yahoo.com/quote/{symbol}/history?period1={date_to_unix(start_date)}&period2={date_to_unix(end_date)}&interval=1d&filter=history&frequency=1d\"\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        rows = driver.find_elements(By.XPATH, '//table[contains(@class, \"W(100%)\")]/tbody/tr')\n",
    "        for row in rows:\n",
    "            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if len(cols) > 0:\n",
    "                date = cols[0].text\n",
    "                close_price = cols[4].text.replace(\",\", \"\")  # Remove commas in the prices\n",
    "                stock_data.append([date, close_price])\n",
    "        return stock_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process the stocks in chunks of 200\n",
    "def process_stocks_in_chunks(driver, stock_symbols, suffix, start_date, end_date):\n",
    "    last_completed = load_last_completed_stock()\n",
    "    error_flag = load_error_flag()\n",
    "\n",
    "    for i in range(last_completed, len(stock_symbols), 200):\n",
    "        chunk_symbols = stock_symbols[i:i + 200]\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        for j, symbol in enumerate(chunk_symbols):\n",
    "            full_symbol = symbol + suffix\n",
    "\n",
    "            # If there was an error last time, remove the previous data and retry\n",
    "            if error_flag and j == 0:  # Start from the last failed stock\n",
    "                remove_previous_data(full_symbol)\n",
    "                error_flag = False\n",
    "                save_error_flag(False)\n",
    "\n",
    "            stock_data = fetch_stock_data(driver, full_symbol, start_date, end_date)\n",
    "            if stock_data is None:\n",
    "                # If there's an error, save the current state and exit\n",
    "                save_error_flag(True)\n",
    "                save_last_completed_stock(i + j)\n",
    "                driver.quit()\n",
    "                return\n",
    "\n",
    "            df = pd.DataFrame(stock_data, columns=['Date', symbol])\n",
    "            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "            df[symbol] = pd.to_numeric(df[symbol], errors='coerce').fillna(0)  # Fill missing values with 0\n",
    "            df.set_index('Date', inplace=True)\n",
    "            data = pd.concat([data, df], axis=1)\n",
    "\n",
    "        # Append the chunk to the CSV file\n",
    "        if os.path.exists(output_file):\n",
    "            existing_data = pd.read_csv(output_file, index_col='Date')\n",
    "            combined_data = pd.concat([existing_data, data], axis=1)\n",
    "            combined_data.to_csv(output_file)\n",
    "        else:\n",
    "            data.to_csv(output_file)\n",
    "        \n",
    "        # Mark this chunk as completed\n",
    "        save_last_completed_stock(i + len(chunk_symbols))\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"All stocks processed successfully.\")\n",
    "\n",
    "# Convert date to Unix timestamp\n",
    "def date_to_unix(date_str):\n",
    "    return int(time.mktime(time.strptime(date_str, '%Y-%m-%d')))\n",
    "\n",
    "# Main function to fetch BSE data\n",
    "def main():\n",
    "    start_date = '2003-04-01'\n",
    "    end_date = '2024-03-31'\n",
    "    bse_stocks = ['RELIANCE', 'TCS', 'INFY']  # Replace with full list of BSE stock symbols\n",
    "\n",
    "    driver = init_driver()\n",
    "    \n",
    "    # Process stocks in chunks of 200\n",
    "    process_stocks_in_chunks(driver, bse_stocks, BSE_SUFFIX, start_date, end_date)\n",
    "\n",
    "if not combined_data_exists:\n",
    "    main()\n",
    "else:\n",
    "    print(\"BSE data exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c00a34",
   "metadata": {},
   "source": [
    "# Function to Update Data\n",
    "Run it only at the end of financial year to get the ending financial year's data and update the combined stock data csv file. Run the functions to get the 10 years and 20 years stocks list data to update them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aac6ec-f623-436e-bee7-b217c9a69b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318bc08-a203-462d-bc1c-db75ec73242a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
